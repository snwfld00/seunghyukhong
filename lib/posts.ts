export interface Post {
  slug: string
  title: string
  date: string
  excerpt: string
  tags: string[]
  author: string
  content: string
}

export const posts: Post[] = [
  {
    "slug": "grounding-dino",
    "title": "Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection",
    "date": "2026-01-21",
    "excerpt": "DINO의 강력한 탐지 능력과 GLIP의 Grounding 학습 방식을 결합하여, 텍스트 쿼리만으로 학습하지 않은 임의의 객체를 제로샷(Zero-Shot)으로 탐지해내는 Open-Set Object Detector입니다.",
    "tags": [
      "Open-Set Object Detection",
      "Grounding",
      "DINO",
      "Multimodal Learning",
      "Transformer",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) (ECCV 2024)\n\n## 1. Abstract & Introduction\n전통적인 객체 탐지기(Object Detector)들은 사전에 정의된 특정 카테고리(예: COCO의 80개 클래스)만을 탐지할 수 있는 **Closed-set** 시스템이었습니다. 새로운 객체를 탐지하려면 데이터를 추가하여 모델을 재학습해야 하는 한계가 있습니다.\n\n최근 연구들은 인간의 언어(Text)를 입력으로 받아, 설명에 부합하는 임의의 객체를 찾아내는 **Open-set Object Detection**으로 나아가고 있습니다. 이 분야의 선구적인 연구인 **GLIP(Grounded Language-Image Pre-training)**은 객체 탐지를 **\"Phrase Grounding\"** 문제(이미지 내의 영역과 텍스트 내의 단어를 매칭하는 문제)로 재정의하여 큰 성공을 거두었습니다.\n\n**Grounding DINO**는 GLIP의 아이디어를 계승하되, Transformer 기반의 SOTA 탐지기인 **DINO** 아키텍처와 결합하여 성능을 극대화했습니다. 단순한 결합을 넘어, 시각 정보와 언어 정보를 초기 단계부터 깊이 있게 융합(Tight Modality Fusion)하여, 학습 과정에서 보지 못한 객체(Unseen Object)에 대해서도 탁월한 일반화 성능을 자랑합니다.\n\n---\n\n## 2. Proposed Methods\n\nGrounding DINO는 **Dual-Encoder** 구조를 기반으로 하며, 이미지와 텍스트 특징을 융합하는 **Feature Enhancer**, 쿼리를 초기화하는 **Language-Guided Query Selection**, 그리고 최종 탐지를 수행하는 **Cross-Modality Decoder**로 구성됩니다.\n\n![Grounding DINO Architecture](/blog/grounding-dino/architecture.png)\n\n*Figure 1: Grounding DINO의 전체 아키텍처. Feature Extraction -> Enhancer -> Query Selection -> Decoder로 이어지는 흐름을 보여줍니다.*\n\n### 2.1 Feature Extraction & Enhancer\n이미지 백본(Swin Transformer)과 텍스트 백본(BERT)에서 추출된 특징은 서로 다른 공간에 존재합니다. 이를 정렬(Alignment)하기 위해 **Feature Enhancer** 모듈을 도입했습니다.\n* **Self-Attention:** 이미지와 텍스트 각각의 내부 문맥(Intra-modality Context)을 파악합니다.\n* **Cross-Attention:** 이미지 특징은 텍스트를, 텍스트 특징은 이미지를 참조하여 상호 정보(Inter-modality Information)를 교환합니다. 이를 통해 \"고양이\"라는 텍스트가 입력되면 이미지 내의 고양이 영역 특징이 강조되는 식의 상호작용이 초기 단계부터 일어납니다.\n\n### 2.2 Language-Guided Query Selection\nDETR 계열 모델의 성능은 **Object Query**의 초기화 품질에 크게 의존합니다. 기존 DINO는 단순히 이미지 특징만을 사용하여 쿼리를 선택했지만, Grounding DINO는 언어 정보를 적극 활용합니다.\n* **Mechanism:** 이미지 특징과 텍스트 특징 간의 내적(Dot Product) 등을 통해 관련성 맵을 계산합니다. 입력된 텍스트와 가장 연관성이 높은(High Relevance) 이미지 영역의 특징을 선별하여 디코더의 쿼리로 초기화합니다.\n* **Effect:** 이는 모델이 \"무엇을 찾아야 하는지\"를 알고 탐색을 시작하게 하므로, 탐지 효율과 정확도를 크게 높여줍니다.\n\n### 2.3 Cross-Modality Decoder\n디코더에서도 시각-언어 융합은 계속됩니다. 각 디코더 레이어는 다음과 같은 구성을 가집니다.\n1.  **Self-Attention:** 쿼리 간의 상호작용.\n2.  **Image Cross-Attention:** 쿼리가 이미지 특징을 참조하여 객체의 위치와 형태를 파악.\n3.  **Text Cross-Attention:** 쿼리가 텍스트 특징을 참조하여 해당 객체가 지시하는 바(예: \"검은색 강아지\")를 정확히 이해하고 있는지 확인.\n4.  **FFN:** 정보 통합 및 업데이트.\n\n이러한 구조는 쿼리가 시각적 단서와 언어적 단서를 지속적으로 비교하고 업데이트하도록 유도하여 정교한 Grounding을 가능하게 합니다.\n\n### 2.4 Sub-sentence Level Representation\n![Sub-sentence Level Representation](/blog/grounding-dino/sub-sentence-level.png)\n\n*Figure 2: 텍스트 프롬프트 처리 방식 비교. Sub-sentence level은 불필요한 의존성을 제거하면서도 세밀한 정보를 유지합니다.*\n\nGLIP 등 기존 연구들은 문장 전체를 하나의 특징으로 인코딩(Sentence Level)하거나, 단어 하나하나를 독립적으로 인코딩(Word Level)하는 방식을 사용했습니다.\n* **Issue:** Sentence Level은 세밀한 정보가 뭉개질 수 있고, Word Level은 \"Hot dog\"와 같이 여러 단어가 합쳐져 하나의 의미를 갖는 경우를 처리하기 어렵거나 불필요한 단어 간 연관성을 학습할 위험이 있습니다.\n* **Solution:** Grounding DINO는 **Sub-sentence Level** 표현을 제안합니다. 이는 명사구(Noun Phrase) 단위로 어텐션 마스크(Attention Mask)를 적용하여, 관련된 단어들끼리만 상호작용하게 하고 무관한 단어들과의 연결은 차단합니다. 이를 통해 단어 간의 불필요한 의존성을 제거하면서도 의미 단위의 정보를 정확하게 보존합니다.\n\n---\n\n## 3. Experiments\n\n### 3.1 Zero-Shot Object Detection\n![Zero-Shot Performance](/blog/grounding-dino/zero-shot-performance.png)\n\n*Figure 3: COCO, LVIS 등 주요 벤치마크에서의 Zero-Shot 성능 비교.*\n\nCOCO 데이터셋으로 학습된 모델을 사용하여, 학습에 포함되지 않은 LVIS(1200개 이상의 카테고리) 데이터셋을 탐지하는 실험에서 압도적인 성능을 보였습니다. 이는 Grounding DINO가 단순히 클래스 이름을 외우는 것이 아니라, **언어와 시각적 특징 간의 관계**를 본질적으로 학습했음을 증명합니다.\n\n### 3.2 Referring Expression Comprehension (REC)\nREC는 \"왼쪽 의자에 앉아 있는 남자\"와 같이 복잡한 수식어가 포함된 텍스트 지시문을 이해하고 해당 객체를 찾는 태스크입니다. Grounding DINO는 RefCOCO/+/g 데이터셋에서 기존 SOTA 모델들을 능가하며, 복잡한 언어 이해 능력을 입증했습니다.\n\n---\n\n## 4. Conclusion\n**Grounding DINO**는 DINO의 강력한 탐지 성능을 Open-set 시나리오로 성공적으로 확장했습니다. 핵심은 시각과 언어라는 이질적인 두 모달리티를 **초기 융합(Early Fusion)부터 심층 융합(Deep Fusion)까지** 긴밀하게 연결한 아키텍처 설계에 있습니다.\n비록 GLIPv2와 달리 Segmentation 마스크를 생성하지 못한다는 한계가 있지만, 텍스트 프롬프트만으로 세상의 모든 객체를 탐지하려는 비전-언어 모델(Vision-Language Model)의 진보를 보여주는 중요한 이정표가 되는 연구입니다."
  },
  {
    "slug": "renet",
    "title": "RENet: Multi-scale Temporal Events Aggregation for RGB-Event Fusion",
    "date": "2025-11-24",
    "excerpt": "단일 스케일 및 단방향 융합의 한계를 극복한 RGB-Event 객체 탐지 네트워크. Multi-scale Temporal Events Aggregation(E-TMA)과 Bi-Directional Calibration(BDC)을 통해 이동 객체 탐지 성능을 극대화했습니다.",
    "tags": [
      "Event Camera",
      "RGB-Event Fusion",
      "Object Detection",
      "Multimodal Learning",
      "Deep Learning",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [RGB-Event Fusion for Moving Object Detection in Autonomous Driving](https://arxiv.org/abs/2209.08323) (ICRA 2023)\n\n## 1. Abstract & Introduction\n이벤트 카메라(Event Camera)는 마이크로초 단위의 높은 시간 해상도를 가지며, 고속으로 이동하는 물체나 급격한 조명 변화(High Dynamic Range)가 있는 환경에서 RGB 카메라의 약점을 완벽하게 보완합니다. 그러나 기존의 RGB-Event Sensor Fusion 연구들은 크게 두 가지 **구조적 한계(Structural Limitations)**를 가지고 있었습니다.\n\n1.  **Single-scale Input Issue:** 이벤트 데이터는 시간적 범위(Temporal Window)에 따라 정보의 특성이 달라집니다. 짧은 시간의 이벤트는 윤곽선 정보가 뚜렷한 반면, 긴 시간의 이벤트는 모션 정보를 포함하지만 블러(Blur)가 발생합니다. 기존 연구들은 단일 스케일 입력만을 사용하여 이러한 다층적인 시간 정보를 충분히 활용하지 못했습니다.\n2.  **Single-directional Fusion Issue:** 대부분의 융합 방식은 이벤트 특징을 단순히 RGB 특징에 더하거나 붙이는(Concatenate) 단방향 방식이었습니다. 이는 두 모달리티 간의 상호 보완적인 정보 교환을 제한하여, 한쪽 데이터의 품질이 낮을 때 전체 성능이 저하되는 원인이 됩니다.\n\n본 논문은 이러한 문제를 해결하기 위해 **RENet**을 제안합니다. RENet은 다중 스케일의 이벤트 정보를 집계하는 **E-TMA** 모듈과, 양방향으로 정보를 보정하는 **BDC** 모듈을 통해 융합의 효율성을 극대화했습니다.\n\n---\n\n## 2. Proposed Methods\n\nRENet의 아키텍처는 크게 인코더, 집계 모듈(E-TMA), 보정 모듈(BDC), 그리고 디코더로 구성됩니다.\n\n### 2.1 E-TMA (Event Temporal Multi-scale Aggregation)\n이벤트 데이터의 비동기적 특성을 최대한 활용하기 위해, 시간 축(Temporal Axis)에서 서로 다른 범위를 가지는 다중 입력을 처리하도록 설계되었습니다.\n\n* **Multi-scale Input Strategy:**\n    * **Short-range Events:** 물체의 **경계(Boundary)** 및 텍스처 정보가 비교적 뚜렷하게 보존됩니다. 정적인 특징 추출에 유리합니다.\n    * **Long-range Events:** 물체의 이동 궤적에 따른 **모션 블러(Motion Blur)**가 포함됩니다. 이는 오히려 이동하는 물체의 존재(Existence)와 방향성을 파악하는 데 중요한 단서(Cue)가 됩니다.\n* **Adaptive Pooling:** 긴 시간 범위의 이벤트일수록 더 큰 커널(Kernel) 사이즈의 풀링을 적용하여, 노이즈를 억제하고 전역적인 모션 문맥을 요약합니다. 이렇게 추출된 다중 스케일 특징들은 채널 축으로 결합되어 풍부한 시간적 정보를 표현합니다.\n\n### 2.2 Discrepant Two-streaming Encoder\nRGB 이미지와 이벤트 프레임은 데이터의 성격이 매우 다릅니다.\n* **RGB:** 조밀한(Dense) 픽셀 값을 가지며 색상과 질감 정보가 풍부합니다. $\\rightarrow$ **ResNet-101** 사용.\n* **Event:** 희소한(Sparse) 활성화(Activation)를 가지며 에지 정보가 주를 이룹니다. $\\rightarrow$ **ResNet-18** 사용.\n본 논문은 이러한 특성 차이를 고려하여 비대칭적인(Discrepant) 듀얼 백본 구조를 채택했으며, 1x1 Convolution을 통해 채널 차원을 일치시킨 후 융합 단계로 전달합니다.\n\n### 2.3 Bi-Directional Calibration (BDC)\n![BDC Module](/blog/renet/bdc-module.png)\n\n*Figure 1: Bi-Directional Calibration 모듈의 구조. 공간 및 채널 어텐션을 통해 상호 정보를 보정합니다.*\n\n단순한 Summation이나 Concatenation 대신, 두 모달리티가 서로를 가이드하고 보정하는 **양방향 어텐션(Bi-directional Attention)** 메커니즘을 제안합니다.\n\n1.  **Coarse-to-Fine Fusion:**\n    * 먼저 1x1 Conv를 통해 각 특징 맵의 가장 유익한(Informative) 정보를 대략적으로(Coarsely) 추출합니다.\n    * 이후 **Spatial Attention**과 **Channel Attention**을 순차적으로 적용하여 특징을 정제합니다.\n2.  **Cross-Calibration:**\n    * RGB 특징에서 추출한 어텐션 맵(Attention Map)을 이벤트 특징에 적용하고, 반대로 이벤트의 어텐션 맵을 RGB에 적용합니다.\n    * 예를 들어, RGB가 조명 문제로 물체를 놓쳤더라도 이벤트의 강한 활성화 신호가 RGB 특징의 해당 영역을 강조(Calibration)해 줄 수 있습니다.\n3.  **Merge:**\n    * 보정된 두 특징($f'_{RGB}, f'_{Event}$)은 학습 가능한 가중치를 가진 Convolution 레이어를 통해 최종적으로 병합되어, 공유된 표현(Shared Representation)을 형성합니다.\n\n---\n\n## 3. Experiments\n\n### 3.1 DSEC-MOD Dataset Construction\n기존의 이벤트 기반 데이터셋(DSEC, MVSEC 등)은 주로 Depth estimation이나 Optical Flow를 위한 것이었으며, 객체 탐지(Object Detection)를 위한 레이블이 부재했습니다. 일부 연구에서 자동 생성한 레이블은 신뢰도가 낮았습니다.\n연구팀은 이를 극복하기 위해 **DSEC-MOD** 데이터셋을 직접 구축했습니다.\n* **Moving Object Detection:** 정지한 물체보다 이동하는 물체(자동차, 보행자 등) 탐지에 초점을 맞추어 수동으로 정밀하게 레이블링을 수행했습니다.\n* **High Quality:** 모션 블러가 심한 영역에 대해서도 이벤트 데이터를 참고하여 정확한 바운딩 박스를 제공합니다.\n\n### 3.2 Quantitative Results\n![Performance Comparison](/blog/renet/performance-comparison.png)\n\n*Figure 2: 다양한 Fusion 방법론과의 성능 비교.*\n\n* **Comparison with SOTA:** RENet은 Frame mAP 및 Video mAP 지표 모두에서 기존의 단방향 Fusion 모델들을 큰 폭으로 상회했습니다.\n* **E-TMA Efficacy:** 단일 스케일만 사용했을 때보다 Multi-scale 이벤트를 사용했을 때 mAP가 유의미하게 상승하여, 시간적 정보 집계의 중요성을 입증했습니다.\n* **BDC Efficacy:** 양방향 보정 모듈을 제거하고 단순 결합했을 경우 성능 하락이 발생했습니다. 이는 BDC가 각 모달리티의 노이즈를 억제하고 유효한 신호를 증폭시키는 역할을 수행함을 보여줍니다.\n\n---\n\n## 4. Conclusion\n**RENet**은 이벤트 카메라의 잠재력을 십분 활용하기 위해 필수적인 두 가지 요소, 즉 **'시간적 다중 스케일(Multi-scale Temporality)'**과 **'양방향 상호작용(Bi-directional Interaction)'**을 성공적으로 모델링했습니다. 특히 직접 구축한 DSEC-MOD 데이터셋은 향후 이동 객체 탐지 및 센서 퓨전 연구의 중요한 벤치마크가 될 것입니다. RENet은 복잡한 도심 환경이나 고속 주행 상황에서 신뢰할 수 있는 객체 탐지 솔루션을 제공합니다."
  },
  {
    "slug": "tumtraf",
    "title": "TUMTraf: A New Benchmark and Fusion Method for Event-based ITS",
    "date": "2025-09-18",
    "excerpt": "조명 조건이 열악한 환경에서도 강건한 객체 탐지를 위해 제안된 새로운 RGB-Event Sensor Fusion 방법론. Targetless Calibration과 Spatiotemporal Late Fusion(STLF)을 통해 ITS 시스템의 신뢰성을 한 단계 높였습니다.",
    "tags": [
      "Event Camera",
      "Sensor Fusion",
      "ITS",
      "Spatiotemporal Fusion",
      "Object Detection",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [TUMTraf Event: Calibration and Fusion Resulting in a Dataset for Roadside Event-Based and RGB Cameras](https://arxiv.org/abs/2401.08474) (IEEE T-IV 2024)\n\n## 1. Abstract & Introduction\n지능형 교통 시스템(ITS, Intelligent Transportation Systems)에서 객체 탐지의 정확도와 신뢰성은 안전과 직결되는 문제입니다. **이벤트 카메라(Event Camera)**는 픽셀의 밝기 변화만을 비동기적으로 감지하여 마이크로초(µs) 단위의 높은 시간 해상도와 넓은 다이내믹 레인지(HDR)를 제공합니다. 이는 모션 블러나 야간, 역광 등 기존 RGB 카메라가 취약한 환경에서 탁월한 성능을 발휘합니다.\n\n그러나 이벤트 카메라는 색상(Color)과 텍스처(Texture) 정보가 부재하다는 본질적인 한계가 있습니다. 따라서 RGB 카메라와의 상호 보완적인 **센서 융합(Sensor Fusion)**이 필수적입니다.\n본 논문은 실제 도로 환경(Roadside)에서 이벤트-RGB 카메라를 활용하기 위한 포괄적인 솔루션을 제안합니다.\n* **Targetless Calibration:** 인프라 특성상 타겟 설치가 어려운 환경을 고려한 자동 보정 기술.\n* **Spatiotemporal Late Fusion (STLF):** 야간 및 악천후 상황에서 탐지 성능을 극대화하는 새로운 융합 알고리즘.\n* **TUMTraf Dataset:** 고품질의 레이블이 포함된 대규모 이벤트-RGB 데이터셋 공개.\n\n---\n\n## 2. Proposed Methods\n\n본 논문은 두 센서 간의 효율적인 융합을 위해 Calibration부터 Fusion까지의 전체 파이프라인을 설계했습니다.\n\n### 2.1 Sensor Calibration (Targetless)\n도로변에 설치된 카메라들은 물리적으로 접근하기 어렵고, 보정을 위한 체커보드(Checkerboard) 등을 설치하기 불가능한 경우가 많습니다. 연구진은 영상 내의 에지(Edge) 정보와 같은 이미지 자체의 특징(Feature)을 활용하여 두 센서 간의 기하학적 관계를 찾아내는 **Targetless Calibration** 기법을 적용하여, 유지 보수의 편의성과 실용성을 확보했습니다.\n\n### 2.2 Fusion Strategies\n연구에서는 세 가지 수준의 융합 전략을 비교하고, 최종적으로 STLF를 제안합니다.\n\n![Fusion Pipeline](/blog/tumtraf/fusion-pipeline.png)\n\n*Figure 1: 제안된 Fusion 파이프라인 개요. (좌) Early Fusion, (우) Late Fusion 접근법.*\n\n#### A. Early Fusion\n* **Method:** 이벤트 스트림을 일정 시간 동안 누적하여 프레임 형태로 변환한 후, RGB 이미지와 채널 단위로 결합(Concatenation)합니다. 이렇게 생성된 4채널(또는 그 이상) 입력을 YOLOv7 모델에 넣어 학습시킵니다.\n* **Pros/Cons:** 구현이 간단하고 모델이 두 모달리티의 상관관계를 직접 학습할 수 있지만, 모달리티 간의 정렬(Alignment) 오차에 민감하며 이벤트 데이터의 희소성(Sparsity)이 희석될 수 있습니다.\n\n#### B. Simple Late Fusion\n* **Method:** RGB와 이벤트 데이터 각각에 대해 독립적으로 YOLOv7 탐지기를 돌립니다. 그 후 출력된 바운딩 박스들을 융합합니다.\n* **Decision Logic:** 두 센서의 탐지 결과가 겹칠 경우(IoU > Threshold), 위치와 크기는 두 박스의 가중 평균으로 결정하되, 객체의 클래스(Class)는 텍스처 정보가 풍부한 RGB의 예측을 따릅니다.\n\n#### C. Spatiotemporal Late Fusion (STLF)\n![STLF Algorithm](/blog/tumtraf/stlf-algorithm.png)\n\n*Figure 2: Spatiotemporal Late Fusion의 알고리즘 흐름도. 추적(Tracking) 정보를 활용하여 일시적인 탐지 실패나 노이즈를 보정합니다.*\n\n단순 Late Fusion은 야간에 RGB 카메라가 아예 객체를 보지 못하거나(False Negative), 헤드라이트 빛 반사 등으로 허위 탐지(False Positive)를 할 때 취약합니다. **STLF**는 시간적 정보(Temporal Context)를 활용하여 이를 해결합니다.\n\n1.  **Tracking Integration:** 각 모달리티의 탐지 결과에 **SORT (Simple Online and Realtime Tracking)** 알고리즘을 적용하여 객체에 고유 ID를 부여하고 궤적을 추적합니다.\n2.  **Trustworthy Object Selection:**\n    * 이벤트 카메라는 조명 변화에 강인하므로, 야간에도 움직이는 물체를 확실하게 포착할 수 있습니다.\n    * 알고리즘은 이벤트 카메라에서 $N$ 프레임 이상 지속적으로 탐지되고 추적된 객체를 **\"신뢰할 수 있는 객체(Trustworthy Object)\"**로 정의합니다.\n3.  **Fusion Logic:**\n    * 만약 현재 시점에 RGB가 객체를 탐지하지 못했더라도, 해당 위치에 'Trustworthy Object'가 존재한다면 최종 결과에 포함시킵니다.\n    * 반대로, RGB가 무언가를 탐지했더라도 이벤트 카메라의 지지(Support)가 없고 과거 궤적과 일치하지 않는다면(False Positive 의심), 이를 과감히 제거합니다.\n    * 이를 통해 **Spatiotemporal(공간+시간)** 정보를 모두 활용한 강건한 의사결정을 내립니다.\n\n---\n\n## 3. Experiments\n\n### 3.1 TUMTraf Dataset\n본 연구를 위해 독일 뮌헨의 실제 교차로와 고속도로 환경에서 수집한 **TUMTraf 데이터셋**을 공개했습니다.\n* **Composition:** 2,000개 이상의 레이블된 이미지와 1,000만 개 이상의 이벤트가 포함되어 있습니다.\n* **Diversity:** 주간, 야간, 비 오는 날 등 다양한 조명 및 기상 조건을 포함하여 ITS 알고리즘 검증에 최적화되어 있습니다.\n\n### 3.2 Performance Evaluation\n![Performance Comparison](/blog/tumtraf/performance-comparison.png)\n\n*Figure 3: 시간대별 및 융합 방법별 성능 비교 (mAP@0.5).*\n\n* **Robustness in Night Scenarios:** 주간에는 RGB 단일 모델도 높은 성능을 보이지만, **야간 환경**에서는 성능이 급격히 저하됩니다. 반면, STLF를 적용한 모델은 야간에도 주간과 거의 대등하거나 훨씬 우수한 성능을 유지했습니다.\n* **Fusion Effectiveness:** 단순 Late Fusion 대비 STLF는 mAP 기준으로 유의미한 성능 향상을 보였으며, 특히 False Positive를 억제하는 데 큰 효과를 보였습니다. Early Fusion은 일부 상황에서는 효과적이었으나, 조명 변화가 극심한 상황에서는 STLF보다 불안정한 모습을 보였습니다.\n\n---\n\n## 4. Conclusion\n**TUMTraf** 연구는 이벤트 카메라가 단순히 RGB의 보조 수단이 아니라, 극한의 환경에서 시스템의 **안전성(Safety)**을 보장하는 핵심 센서임을 증명했습니다.\n특히 제안된 **Spatiotemporal Late Fusion**은 단일 프레임 정보에만 의존하던 기존 융합 방식의 한계를 넘어, 시간적 궤적 정보를 결합함으로써 ITS 분야에서 센서 퓨전이 나아가야 할 방향을 제시했습니다. 이는 자율주행 차량뿐만 아니라 스마트 시티 인프라 구축에도 즉시 적용 가능한 실용적인 기술입니다."
  },
  {
    "slug": "yolov12",
    "title": "YOLOv12: Attention-Centric Real-Time Object Detector",
    "date": "2025-08-14",
    "excerpt": "CNN의 한계를 넘어 Attention 중심의 아키텍처로 진화한 YOLOv12. Area Attention과 R-ELAN 설계를 통해 계산 복잡도를 획기적으로 낮추면서도 SOTA 성능과 실시간 속도를 동시에 달성했습니다.",
    "tags": [
      "YOLO",
      "Object Detection",
      "Attention Mechanism",
      "Real-Time",
      "Deep Learning",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [YOLOv12: Attention-Centric Real-Time Object Detectors](https://arxiv.org/abs/2502.12524) (NeurIPS 2025)\n\n## 1. Abstract & Introduction\n\n실시간 객체 탐지(Real-Time Object Detection) 분야에서 **YOLO(You Only Look Once)** 프레임워크는 독보적인 위치를 차지해 왔습니다. 그동안 YOLO 시리즈의 발전은 주로 손실 함수(Loss Function), 레이블 할당(Label Assignment), 그리고 CNN 기반의 아키텍처 최적화에 집중되어 왔습니다.\n\n하지만 최근 컴퓨터 비전 분야는 **Vision Transformer(ViT)**가 주도하고 있습니다. Attention 메커니즘은 전역적인 문맥(Global Context)을 파악하는 능력이 뛰어나 CNN보다 높은 정확도를 보이지만, 실시간성이 중요한 객체 탐지에서는 치명적인 단점이 존재합니다.\n\n* **이차적 계산 복잡도(Quadratic Complexity):** 입력 시퀀스 길이의 제곱($O(L^2)$)에 비례하는 연산량.\n* **비효율적인 메모리 접근:** 불규칙한 메모리 패턴으로 인한 높은 지연 시간(Latency).\n\n이로 인해 기존에는 \"속도는 CNN, 성능은 Attention\"이라는 트레이드오프가 존재했습니다. **YOLOv12**는 이러한 통념을 깨고, **Attention 중심(Attention-Centric)**의 설계를 통해 CNN 수준의 빠른 속도를 유지하면서도 Attention 기반 모델의 우수한 성능을 달성하는 것을 목표로 합니다.\n\n---\n\n## 2. Proposed Methods\n\nYOLOv12의 핵심 기여는 기존 Attention의 비효율성을 개선한 **Area Attention**과, 심층 신경망의 최적화 문제를 해결한 **R-ELAN** 구조입니다.\n\n### 2.1 Area Attention\n\n기존의 고해상도 이미지를 처리하는 ViT 모델들은 연산량을 줄이기 위해 **Window-based Attention**을 사용했습니다. 하지만 이 방식은 윈도우 분할(Partitioning) 및 병합 과정에서 오버헤드가 발생하고, 수용 필드(Receptive Field)가 국소적으로 제한되는 단점이 있습니다.\n\n![Area Attention](/blog/yolov12/area-attention.png)\n\nYOLOv12는 **Area Attention**을 제안합니다.\n\n* **Reshape-based Approach:** 복잡한 윈도우 분할 대신, 단순히 Feature Map을 넓은 영역(Area) 단위로 **Reshape**하여 Attention을 수행합니다. 예를 들어, 가로/세로를 일정 비율로 축소하여 키/밸류(Key/Value)를 생성함으로써 넓은 영역의 정보를 적은 연산으로 집약합니다.\n\n* **Efficient Global Context:** 이를 통해 추가적인 메모리 복사나 전처리 오버헤드 없이 넓은 수용 필드를 확보하며, GPU 하드웨어에서의 병렬 처리 효율을 극대화합니다.\n\n### 2.2 R-ELAN (Residual Efficient Layer Aggregation Networks)\n\nYOLO 시리즈에서 널리 사용되던 ELAN(Efficient Layer Aggregation Networks) 구조는 네트워크가 깊어질수록 학습 시 Gradient 흐름이 원활하지 않아 수렴이 불안정해지는 문제가 있었습니다.\n\n![R-ELAN Structure](/blog/yolov12/r-elan.png)\n\n**R-ELAN**은 이를 해결하기 위해 두 가지 개선을 도입했습니다.\n\n1.  **Block-wise Residual Shortcut:** 블록의 입력에서 출력으로 바로 연결되는 Shortcut을 추가하되, **Scale Factor (기본값 0.01)**를 곱하여 더해줍니다. 이는 DeepViT의 LayerScale과 유사한 원리로, 초기 학습 시에는 Identity Mapping에 가깝게 동작하여 깊은 망에서도 안정적인 학습을 보장합니다.\n\n2.  **Simplified Aggregation:** 불필요한 Feature Split 과정을 제거하고 단일 Feature Map을 사용하여, 메모리 사용량과 파라미터 수를 줄이면서도 정보 전달 효율을 높였습니다.\n\n### 2.3 Architectural Improvements\n\n* **Positional Encoding 제거:** 기존 ViT는 위치 정보를 위해 Positional Encoding을 더해주어야 했지만, YOLOv12는 이를 제거했습니다. 대신 **7x7 Convolution**을 추가하거나 Area Attention 구조 내에서 위치 정보가 암묵적으로 학습되도록 유도하여 구조를 단순화했습니다.\n\n* **FlashAttention 적용:** 최신 GPU에 최적화된 Attention 커널인 FlashAttention을 도입하여 메모리 접근 속도를 비약적으로 향상시켰습니다.\n\n* **Optimized Backbone:** 백본의 마지막 스테이지를 단일 R-ELAN 블록으로 대체하여 전체적인 연산 비용을 최적화했습니다.\n\n---\n\n## 3. Experiments\n\n### 3.1 Comparison with SOTA\n\n![COCO Performance Comparison](/blog/yolov12/coco-comparison.png)\n\n실험 결과, YOLOv12는 기존 실시간 탐지기(YOLOv8, v10 등) 및 Attention 기반 모델(RT-DETR 등)을 모두 압도하는 성능을 보였습니다.\n\n* 동일한 연산 예산(Computational Budget) 하에서 CNN 기반 모델보다 최대 **3배** 더 효율적인 성능 곡선을 보여줍니다.\n\n* 특히 L(Large), X(Extra-large) 스케일의 대형 모델에서도 AdamW 옵티마이저를 사용하여 안정적으로 수렴함이 확인되었습니다.\n\n### 3.2 Ablation Study\n\n![Ablation Study](/blog/yolov12/ablation-study.png)\n\n* **Area Attention의 효과:** 일반적인 Global Attention이나 Window Attention을 사용했을 때보다 Area Attention을 적용했을 때 속도 저하 없이 정확도가 상승했습니다.\n\n* **R-ELAN의 효과:** Residual Shortcut과 Scale Factor가 없을 경우, 특히 대형 모델에서 학습이 발산하거나 성능이 저하되는 현상이 발생했습니다. R-ELAN은 이를 효과적으로 방지했습니다.\n\n---\n\n## 4. Conclusion\n\n**YOLOv12**는 \"실시간 탐지에는 CNN이 유리하다\"는 고정관념을 깨뜨렸습니다. **Area Attention**을 통해 Attention 메커니즘의 연산 비효율을 극복하고, **R-ELAN**을 통해 깊은 네트워크의 최적화 난이도를 낮췄습니다. 결과적으로 YOLOv12는 속도와 정확도라는 두 마리 토끼를 모두 잡으며, 차세대 실시간 객체 탐지의 새로운 표준을 제시했습니다. 다만, 여느 YOLO 시리즈와 마찬가지로 극소형 객체(Tiny Objects)에 대한 탐지 성능은 지속적인 개선이 필요합니다."
  },
  {
    "slug": "rt-detr",
    "title": "RT-DETR: DETRs Beat YOLOs on Real-time Object Detection",
    "date": "2025-07-01",
    "excerpt": "YOLO의 NMS로 인한 병목 현상과 DETR의 높은 연산 비용 문제를 동시에 해결한 최초의 실시간 End-to-End 객체 탐지기. Efficient Hybrid Encoder와 Uncertainty-minimal Query Selection을 제안합니다.",
    "tags": [
      "Real-Time Object Detection",
      "DETR",
      "YOLO",
      "End-to-End",
      "Deep Learning",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/abs/2304.08069) (CVPR 2024)\n\n# 1. Abstract & Introduction\n\n실시간 객체 탐지(Real-Time Object Detection) 분야는 오랫동안 **YOLO** 시리즈가 주도해 왔습니다. 그러나 YOLO와 같은 CNN 기반 탐지기들은 **NMS(Non-Maximum Suppression)** 후처리 과정이 필수적입니다. NMS는 추론 속도를 지연시킬 뿐만 아니라, 하이퍼파라미터(Threshold)에 따라 성능이 민감하게 변하는 불안정성을 야기합니다.\n\n반면, **DETR(DEtection TRansformer)** 계열은 NMS 없는(NMS-free) End-to-End 학습이 가능하지만, 높은 연산 비용으로 인해 실시간 처리에 한계가 있었습니다. 특히 다중 스케일(Multi-scale) 특징을 활용할 때 입력 시퀀스의 길이가 폭발적으로 증가하여 인코더에서 병목 현상이 발생합니다.\n\n본 논문은 이러한 두 가지 접근법의 한계를 극복한 **RT-DETR**을 제안합니다.\n\n* **Efficient Hybrid Encoder:** 다중 스케일 특징의 상호작용을 효율적으로 분리하여 연산 속도를 획기적으로 개선했습니다.\n* **Uncertainty-minimal Query Selection:** 인코더 특징으로부터 고품질의 초기 쿼리를 선별하여 탐지 정확도를 높였습니다.\n\n---\n\n# 2. Proposed Methods\n\nRT-DETR의 전체 아키텍처는 Backbone, Efficient Hybrid Encoder, 그리고 Transformer Decoder로 구성됩니다. 핵심 기술인 인코더와 쿼리 선택 모듈을 상세히 설명합니다.\n\n![RT-DETR Pipeline](/blog/rt-detr/pipeline.png)\n\n## 2.1 Efficient Hybrid Encoder\n\n기존 DETR 모델들은 모든 스케일의 특징을 평탄화(Flatten)하여 하나의 긴 시퀀스로 만든 후 Attention을 수행했습니다. 이는 이미지 해상도가 높을수록 연산량이 제곱으로 증가하는 원인이 됩니다. RT-DETR은 이를 해결하기 위해 **Intra-scale Interaction**과 **Cross-scale Fusion**을 분리(Decoupling)하는 전략을 취합니다.\n\n![Efficient Hybrid Encoder](/blog/rt-detr/hybrid-encoder.png)\n\n### A. Attention-based Intra-scale Feature Interaction (AIFI)\n\n상위 레벨의 특징(High-level features, $S_5$)은 이미지 내의 객체에 대한 풍부한 의미(Semantic) 정보를 담고 있습니다. 반면, 하위 레벨 특징($S_3, S_4$)은 엣지나 패턴 같은 공간적 세부 정보를 포함합니다.\n\n* RT-DETR은 연산 비용이 높은 **Self-Attention 연산을 오직 $S_5$ 스케일에만 적용**합니다. ($S_5$는 해상도가 낮아 시퀀스 길이가 짧으므로 연산 부담이 적음)\n* 이를 통해 객체 간의 전역적인 관계(Global Context)를 효율적으로 포착하면서도 불필요한 저수준 특징 간의 상호작용을 배제하여 연산 효율을 극대화합니다.\n\n### B. CNN-based Cross-scale Feature Fusion (CCFF)\n\n서로 다른 스케일의 특징들을 융합하는 과정에서는 CNN 기반 구조를 활용합니다.\n\n* AIFI를 통과한 $S_5$ 특징과 나머지 $S_3, S_4$ 특징을 융합하기 위해 **Fusion Block**을 설계했습니다.\n* 각 융합 단계에서는 **RepBlock** (Reparameterization Block, YOLO 등에서 사용되는 효율적인 Conv 블록)을 사용하여 특징 맵을 효과적으로 섞고 채널을 조정합니다.\n* 이 방식은 Transformer의 복잡한 Attention 없이도 다중 스케일 특징을 효과적으로 통합할 수 있게 해줍니다.\n\n## 2.2 Uncertainty-minimal Query Selection\n\nDETR의 성능은 디코더에 입력되는 **Object Query**의 품질에 크게 좌우됩니다. 기존 모델들은 인코더 특징 중 **분류 점수(Classification Score)**가 높은 상위 $K$개의 특징을 선택하여 쿼리로 사용했습니다.\n\n![Uncertainty-minimal Query Selection](/blog/rt-detr/query-selection.png)\n\n* **문제점:** 분류 점수가 높다고 해서 해당 특징이 객체의 **정확한 위치(Localization)**를 담고 있다고 보장할 수 없습니다. 즉, 분류 점수와 위치 정확도 간의 불일치(Misalignment)가 존재합니다.\n\n* **해결책:** RT-DETR은 **Uncertainty-minimal Query Selection**을 제안합니다. 이는 쿼리 선택 시 분류 점수뿐만 아니라, 예측된 박스의 **위치 불확실성(IoU Score 등)**을 함께 고려하는 방식입니다.\n\n* 분류 점수와 예측된 IoU 품질을 결합한 점수를 기준으로 상위 $K$개를 선택함으로써, 디코더가 더 정확한 초기 위치 정보와 의미 정보를 가지고 탐지를 시작할 수 있도록 합니다. 이는 모델의 수렴 속도를 높이고 최종 성능을 향상시키는 데 기여합니다.\n\n---\n\n# 3. Experiments\n\n## 3.1 Quantitative Analysis\n\n![COCO Performance Comparison](/blog/rt-detr/coco-comparison.png)\n\n* **Comparison with YOLO:** RT-DETR-L은 **53.0% AP / 114 FPS**를 기록하며, YOLOv8-L (52.9% AP / 75 FPS) 대비 정확도는 유사하지만 속도는 약 50% 더 빠릅니다. RT-DETR-X 역시 YOLOv8-X보다 높은 정확도와 빠른 속도를 달성했습니다.\n\n* **Comparison with DETR:** 기존 SOTA인 DINO-Deformable-DETR-R50과 비교했을 때, RT-DETR-R50은 정확도는 2.2% AP 더 높으면서 속도는 무려 21배 빠릅니다.\n\n## 3.2 Ablation Study\n\n![Ablation Study](/blog/rt-detr/ablation-study.png)\n\n* **Encoder Variants:** AIFI를 $S_5$에만 적용하고 CCFF를 사용하는 구성(DS5 + CCFF)이, 모든 스케일에 Attention을 적용하거나 단순히 Concat하는 방식보다 성능과 속도 면에서 가장 우수한 균형을 보였습니다.\n\n* **Query Selection:** Uncertainty-minimal Query Selection을 적용했을 때, 단순 Classification Score 기반 선택보다 AP가 유의미하게 향상됨을 확인했습니다.\n\n---\n\n# 4. Conclusion\n\nRT-DETR은 **\"실시간 객체 탐지 = YOLO\"**라는 공식을 깨고, Transformer 기반 모델도 실시간 처리가 가능함을 증명했습니다.\n\n1.  **NMS-free:** 후처리 과정을 제거하여 추론 속도의 안정성을 확보했습니다.\n\n2.  **Architecture Efficiency:** Intra-scale과 Cross-scale 처리를 분리한 하이브리드 인코더 설계를 통해 연산 효율을 극대화했습니다.\n\n3.  **Accuracy:** 불확실성을 고려한 쿼리 선택으로 초기화 품질을 높여 SOTA 성능을 달성했습니다.\n\n이 연구는 자율주행, 로보틱스 등 실시간성과 정확도가 모두 요구되는 응용 분야에서 YOLO의 강력한 대안이 될 것입니다."
  },
  {
    "slug": "semi-detr",
    "title": "Semi-DETR: Semi-Supervised Object Detection with Stage-wise Hybrid Matching",
    "date": "2025-05-19",
    "excerpt": "DETR의 One-to-One 매칭이 갖는 Semi-Supervised Learning(SSL) 환경에서의 구조적 한계를 분석하고, 이를 극복하기 위한 Stage-wise Hybrid Matching 및 Cross-view Query Consistency 메커니즘을 제안한 연구입니다.",
    "tags": [
      "Semi-Supervised Learning",
      "Object Detection",
      "DETR",
      "Bipartite Matching",
      "Pseudo-Labeling",
      "Deep Learning",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [Semi-DETR: Semi-Supervised Object Detection with Detection Transformers](https://arxiv.org/abs/2307.08095) (CVPR 2023)\n\n## 1. Introduction: The Dilemma of DETR in SSOD\n\n최근 Object Detection 분야는 CNN 기반의 One-stage/Two-stage 탐지기에서 Transformer 기반의 DETR(DEtection TRansformer)로 패러다임이 전환되고 있습니다. 그러나 레이블이 부족한 환경인 **Semi-Supervised Object Detection (SSOD)** 태스크에서 DETR을 단순히 적용하는 데에는 근본적인 구조적 한계가 존재합니다.\n\n### 1.1 The Conflict: One-to-One Assignment vs. Noisy Pseudo-Labels\n\n기존 CNN 기반 모델(RetinaNet, YOLO 등)은 **One-to-Many Assignment** 전략을 사용합니다. 하나의 Ground Truth(GT)에 대해 여러 개의 Anchor Box를 양성(Positive)으로 할당하기 때문에, Teacher 모델이 생성한 Pseudo-label에 다소 노이즈가 있더라도 True Positive를 포함할 확률(Recall)이 높아 학습이 안정적입니다.\n\n반면, DETR은 **One-to-One Bipartite Matching**을 수행합니다. 이는 GT와 예측값 간의 엄격한 1:1 대응을 강제하여 NMS(Non-Maximum Suppression) 없는 End-to-End 학습을 가능하게 합니다. 하지만 SSOD의 Teacher-Student 프레임워크에서는 이 장점이 치명적인 약점이 됩니다.\n\n* **Teacher의 오류 전파:** Teacher가 생성한 Pseudo-label은 부정확할 수 있습니다(Noisy Labels).\n* **True Negative 유발:** 만약 Teacher가 놓친 객체나 부정확한 박스를 GT로 간주하고 1:1 매칭을 수행하면, Student 모델이 예측한 나머지 타당한 후보들은 모두 강제로 **배경(Negative)**으로 처리됩니다. 이는 모델이 올바른 특징을 학습하는 것을 방해하고 수렴을 저해합니다.\n\n본 논문은 이러한 **\"불완전한 Pseudo-label 하에서의 비효율적인 1:1 매칭 문제\"**를 해결하기 위해 **Semi-DETR**을 제안합니다.\n\n---\n\n## 2. Methodology\n\nSemi-DETR은 기본적으로 Teacher-Student 구조를 따르며, Teacher 네트워크는 Student 네트워크의 **EMA(Exponential Moving Average)**로 업데이트됩니다. 핵심 기여는 다음 세 가지 모듈로 구성됩니다.\n\n### 2.1 Stage-wise Hybrid Matching\n\n본 논문은 One-to-Many 방식의 높은 Recall과 One-to-One 방식의 정교한 Localization 능력을 결합한 **단계적 하이브리드 매칭(Stage-wise Hybrid Matching)** 전략을 제안합니다.\n\n![Stage-wise Hybrid Matching](/blog/semi-detr/stage-wise-hybrid-matching.png)\n\n1.  **Stage 1: One-to-Many Pairing (Candidate Generation)**\n    * 먼저 Teacher가 생성한 각 Pseudo-label에 대해, Student의 예측값 중 매칭 비용(Matching Cost)이 낮은 상위 $M$개의 후보군(Candidate Set)을 할당합니다.\n    * 이 단계는 CNN의 Anchor Assignment와 유사하게 작동하며, 노이즈가 섞인 레이블에 대해 모델이 더 넓은 범위의 특징을 탐색할 수 있도록 유도합니다(Recall 증대).\n\n2.  **Stage 2: One-to-One Pairing (Final Assignment)**\n    * 선별된 $M$개의 후보군 내에서 헝가리안 알고리즘(Hungarian Algorithm)을 수행하여 최종적으로 가장 적합한 단 하나의 예측만을 선택합니다.\n    * 이를 통해 DETR 고유의 1:1 매칭 특성을 유지하며 중복 탐지를 방지합니다.\n\n이 과정에서 분류 점수(Classification Score)와 박스 회귀(Box Regression) 손실의 가중치를 동적으로 조절하여, 초기 학습 단계에서의 불안정성을 완화하고 점진적으로 정교한 매칭이 이루어지도록 설계되었습니다.\n\n### 2.2 Cross-view Query Consistency\n\nSSOD의 핵심은 **Consistency Regularization**입니다. 하지만 DETR의 Object Query는 학습된 위치 정보를 포함하고 있어, 입력 이미지의 뷰(View)가 달라지면 쿼리의 의미적 매칭이 깨질 수 있습니다.\n\n![Cross-view Query Consistency](/blog/semi-detr/cross-view-query-consistency.png)\n\n* **문제점:** 동일한 이미지 $I$에 대해 서로 다른 증강(Augmentation)을 적용한 뷰 $V_1, V_2$가 있을 때, DETR은 동일한 객체라도 뷰에 따라 서로 다른 인덱스의 Query에 할당할 수 있습니다. 기존의 고정된 인덱스 매칭은 이러한 변화를 반영하지 못합니다.\n\n* **해결책:** Semi-DETR은 **Cross-view Query Consistency** 모듈을 통해, 뷰가 다르더라도 동일한 객체를 가리키는 Query들의 **Feature Embedding**이 일관성을 갖도록 학습합니다. 이는 뷰 간의 고정된 매칭 제약을 제거하고, 의미적 유사성(Semantic Similarity)을 기반으로 유연하게 일관성 손실(Consistency Loss)을 계산함으로써 달성됩니다.\n\n### 2.3 Cost-based Pseudo Label Mining\n\nCross-view Consistency 학습이 효과적이려면, Unlabeled 데이터에서 추출한 Pseudo-label의 품질이 보장되어야 합니다. 기존의 고정된 Confidence Thresholding 방식은 데이터 분포의 변화에 유연하게 대처하지 못합니다.\n\n![Cost-based Pseudo Label Mining](/blog/semi-detr/cost-based-pseudo-label-mining.png)\n\n본 논문은 **GMM(Gaussian Mixture Model)**을 활용하여 매칭 비용(Matching Cost)의 분포를 모델링합니다.\n\n* 모든 예측의 매칭 비용을 수집하여 GMM으로 피팅하면, **신뢰할 수 있는 클러스터(Reliable Cluster)**와 **노이즈 클러스터(Unreliable Cluster)**로 구분할 수 있습니다.\n* 이 분포를 기반으로 불확실성이 낮은(Low-uncertainty) Pseudo-label만을 동적으로 선별(Mining)하여 학습에 반영합니다. 이는 하이퍼파라미터 튜닝의 의존도를 낮추고 레이블 품질을 보장하는 역할을 합니다.\n\n---\n\n## 3. Experimental Results\n\n### 3.1 Benchmark Performance (COCO)\n\nCOCO 데이터셋을 기준으로 **Partial Label** (1%, 5%, 10% 레이블) 및 **Full Label** 설정에서 실험을 수행했습니다.\n\n![COCO Benchmark Results](/blog/semi-detr/coco-benchmark-results.png)\n\n* **SOTA 달성:** Semi-DETR은 모든 레이블 비율 설정에서 STAC, Unbiased Teacher, Soft Teacher 등 기존의 강력한 CNN 기반 SSOD 모델들을 능가했습니다. 특히 1%의 극소량 레이블 설정에서도 mAP가 크게 향상되어, 제안하는 하이브리드 매칭 전략이 데이터 희소성(Data Scarcity) 문제에 매우 효과적임을 입증했습니다.\n\n* **수렴 속도:** 기존 DETR 기반 방법론 대비 학습 수렴 속도가 빠르며, 이는 Stage-wise Hybrid Matching이 초기 학습의 난이도를 낮춰주는 덕분인 것으로 분석됩니다.\n\n### 3.2 Ablation Study\n\n각 컴포넌트의 기여도를 분석한 결과는 다음과 같습니다.\n\n![Ablation Study](/blog/semi-detr/ablation-study.png)\n\n* **Hybrid Matching의 효과:** 단순 1:1 매칭만 사용했을 때보다 1:Many + 1:1 매칭을 결합했을 때 성능이 비약적으로 상승했습니다.\n\n* **Consistency의 중요성:** Cross-view Query Consistency를 제거하면 성능 하락이 발생하였으며, 이는 DETR 구조에서 쿼리 임베딩의 일관성 학습이 필수적임을 시사합니다.\n\n* **Cost-based Mining:** GMM 기반의 동적 마이닝 방식이 고정 임계값 방식보다 더 강건한 성능을 보였습니다.\n\n---\n\n## 4. Conclusion & Insight\n\n**Semi-DETR**은 DETR을 SSOD에 적용할 때 발생하는 근본적인 문제인 \"부정확한 레이블과 엄격한 1:1 매칭의 충돌\"을 논리적으로 파고들었습니다.\n\n1.  **Hybrid Matching:** CNN의 장점(Recall)과 DETR의 장점(Precision)을 단계적으로 결합하는 영리한 전략을 제시했습니다.\n\n2.  **Feature Robustness:** 이미지 뷰의 변화에 흔들리지 않는 강건한 Query Feature를 학습하는 방법을 고안했습니다.\n\n이 연구는 향후 Transformer 기반의 Semi-Supervised Learning 연구들이 어떻게 Label Noise를 다루어야 하는지에 대한 중요한 가이드라인을 제시합니다. NMS-free 구조의 장점을 유지하면서도 데이터 효율성을 극대화했다는 점에서 높은 학술적 가치를 지닙니다."
  },
  {
    "slug": "leod_label-efficient-object-detection-for-event-cameras",
    "title": "LEOD: Label-Efficient Object Detection for Event Cameras",
    "date": "2025-05-06",
    "excerpt": "이벤트 카메라를 위한 Semi-supervised Object Detection 논문, Pseudo-label을 활용하여 적은 레이블 데이터로 효율적인 학습을 달성하는 방법에 대한 연구입니다.",
    "tags": [
      "Event Camera",
      "Object Detection",
      "Semi-Supervised Learning",
      "Paper Review"
    ],
    "author": "SeungHyuk Hong",
    "content": "> **Paper:** [LEOD: Label-Efficient Object Detection for Event Cameras](https://arxiv.org/abs/2311.17286) (CVPR 2024)\n\n# Abstract & Introduction\n  Exploits the **low latency** and **high dynamic range** of event cameras for fast and precise object detection.\n  High temporal resolution makes per-timestamp labeling costly (e.g., the Gen1 dataset provides labels at only <4 Hz).\n  Uses a pre-trained detector on limited labeled data to generate **pseudo-labels** for unlabeled events, then retrains by combining true and pseudo labels. Enhances pseudo-label quality through temporal consistency, bidirectional inference, and tracking-based post-processing. \n  On the Gen1 dataset, RVT-S using 1% and 2% labels shows mAP improvements of 8.6% and 7.8%, respectively. On the 1Mpx dataset, using only 10% labels achieves performance comparable to fully supervised training (100% labels). \n  Object detection transforms raw sensor data into meaningful bounding boxes—a critical task in safety-critical applications like autonomous driving where speed is essential. LEOD provides a novel direction by leveraging event cameras’ advantages while innovatively addressing the labeling challenge.\n  LEOD is based on [RVT](https://arxiv.org/abs/2212.05598) model.\n  \n# Backgrounds\n### Pseudo Labeling\n\n![](https://velog.velcdn.com/images/snwfld/post/c8027ead-9870-415b-b7ec-2947ad6fb568/image.png)\nLEOD applied pseudo-labeling to **semi-supervised learning** to improve model performance by minimizing reliance on manually annotated data while exploiting unlabeled event streams.\n### Test Time Augmentation\n\n![](https://velog.velcdn.com/images/snwfld/post/6124e838-5a3b-485c-bf5a-47ef61270bff/image.png)\nTest Time Augmentation (TTA) enhanced model performance by generating multiple augmented versions of the input image at inference time. The model makes predictions on each version, and these are aggregated to produce a more robust and accurate final prediction.\n# Methods\n![](https://velog.velcdn.com/images/snwfld/post/713b1ec0-f29f-4940-9919-fd2080b42278/image.png)\nLEOD is trained based on the RVT model through the process shown in the figure above.\n- **Pre-train** an event-based object detector on event streams with limited labels.\n- **TTA & Ensemble**: Test-Time Augmentation (TTA) is applied by flipping the temporal sequence of events, and predictions from both forward and backward event streams are ensembled.\n- **Filter & Track**: Forward and backward tracking is used to identify and remove temporally inconsistent bounding boxes, such as those associated with short tracks.\n- **Soft Anchor Assignment**: Adjust loss function application to handle incomplete Pseudo Labels label noise, such as accompanying noise.\n- **Self-Training**: The model is retrained using reliable pseudo labels, and steps 1–4 are repeated to progressively enhance performance.\n# Experiments\n![](https://velog.velcdn.com/images/snwfld/post/596a0283-90c2-472e-b4ee-6f30c90d6e7b/image.png)\n**Result**: \n![](https://velog.velcdn.com/images/snwfld/post/fe7ab0d7-708a-41d4-9436-cd5da7ba0f85/image.png)\n![](https://velog.velcdn.com/images/snwfld/post/64c33b69-b782-4848-b48c-90751aa6f8b9/image.png)\n![](https://velog.velcdn.com/images/snwfld/post/8940284c-db6c-4745-a94a-6ea8e6594651/image.png)\n![](https://velog.velcdn.com/images/snwfld/post/84a03999-15c5-4221-9fff-0518ff405739/image.png)\n# Conclusion\nLEOD introduces multiple techniques to enhance pseudo-label quality. As a result, extensive experiments on Gen1 and 1Mpx datasets show LEOD outperforms baselines across all label settings.\n\nExperiments have limitation that is conducted only intra-dataset (using data from the same protocol).\n\nThose are future Directions.\n- Joint training on multiple, real-world multi-object event sequence datasets.\n- Leverage unlabeled data to improve performance and generalization further."
  },
  {
    "slug": "gdgoc-1기-core-멤버-활동-시작",
    "title": "GDGoc 1기 Core 멤버 활동 시작",
    "date": "2024-09-06",
    "excerpt": "",
    "tags": [
      "GDG",
      "Computer Vision"
    ],
    "author": "SeungHyuk Hong",
    "content": "![](/blog/image.png)\n\n드디어 우리 학교에도 GDGoc가 생겼고, 멤버 모집 과정부터 대략적인 로드맵 설계 등으로 인해 정신없는 8월 말 ~ 9월 초를 보냈다. 아직 시도때도 없이 아쉽고, 보완해야 할 부분들이 나오고 있지만, 부디 내년 이맘때면 이 동아리가 순탄히 굴러가고 있길 바랄 뿐이다. 현재 운영진도 하며 tech 파트에도 참여하고 있는데 솔직히 생각보다 힘겹지만 이 또한 버티면 지나가지 않을까 싶다. 내가 이 동아리에서 졸업하게 될 때는 누구나 오고 싶어하는 GDGoc가 될 수 있도록 보탬이 되면 좋겠다. 우리 운영진이랑 Tech Core 멤버들이 이 글을 보시긴 힘들겠지만 한 해 동안 화이팅..!"
  }
]

export function getPostBySlug(slug: string): Post | null {
  return posts.find((post) => post.slug === slug) || null
}

export function getAllPosts(): Post[] {
  return posts.sort((a, b) => new Date(b.date).getTime() - new Date(a.date).getTime())
}
